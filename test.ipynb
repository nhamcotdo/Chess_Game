{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from abc import ABC, abstractmethod\n",
    "from tensorflow.keras import Sequential\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import losses\n",
    "from tensorflow.keras import optimizers\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BaseModel(ABC):\n",
    "    @abstractmethod\n",
    "    def __init__(self, discount_factor: float, epsilon: float, e_min: int, e_max: int):\n",
    "        \"\"\"\n",
    "        self.qNetwork and self.qTargetNetwork need to be installed.\n",
    "\n",
    "        :param discount_factor: It’s used to balance immediate and future reward.\n",
    "        Typically this value can range anywhere from 0.8 to 0.99.\n",
    "\n",
    "        :param epsilon: the probability of choosing to explore action.\n",
    "        :param e_min: Minimum amount of experience to start training.\n",
    "        :param e_max: Maximum amount of experience.\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def observe(self, state, action_space: list = None):\n",
    "        \"\"\"\n",
    "        Observe state from environment and return a action\n",
    "\n",
    "        :param state: Current situation returned by the environment.\n",
    "        :param action_space: All the possible moves that the agent can take.\n",
    "        :return: Action that have the max value from q table value.\n",
    "\n",
    "        Note: If actionSpace is None then all action are possible\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def observe_on_training(self, state, action_space: list = None) -> int:\n",
    "        \"\"\"\n",
    "        Observe state from environment and return a action by epsilon greedy policy.\n",
    "        The state and action will be stored in the memory buffer to be used for Experience Replay\n",
    "\n",
    "        :param state: Current situation returned by the environment.\n",
    "        :param action_space: All the possible moves that the agent can take.\n",
    "        :return: Action that have the max value from q table value.\n",
    "        Note: If actionSpace is None then all action are possible\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def take_reward(self, reward, next_state, done):\n",
    "        \"\"\"\n",
    "        After used observeOnTraining method, environment will return reward, nextState and done information\n",
    "        we will use this method to put that information into the Experience Replay\n",
    "\n",
    "        :param reward: immediate reward returned by the environment\n",
    "        :param next_state: Next situation returned by the environment.\n",
    "        :param done: describes whether the environment situation has terminated or not\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def train_network(self, sample_size: int, batch_size: int, epochs: int, verbose: int = 2, cer_mode: bool = False):\n",
    "        \"\"\"\n",
    "        :param sample_size: number of samples taken from Experience Replay.\n",
    "        :param batch_size: Integer or `None`. Number of samples per gradient update. If unspecified,\n",
    "        `batch_size` will default to 32. Do not specify the `batch_size` if your data is in the form of datasets,\n",
    "        generators, or `keras.utils.Sequence` instances (since they generate batches).\n",
    "        :param epochs: Integer. Number of epochs to train the model.\n",
    "        An epoch is an iteration over the entire `x` and `y` data provided.\n",
    "        Note that in conjunction with `initial_epoch`, `epochs` is to be understood as \"final epoch\".\n",
    "        The model is not trained for a number of iterations given by `epochs`, but merely until the epoch of index\n",
    "        `epochs` is reached.\n",
    "        :param verbose: 0, 1, or 2. Verbosity mode. 0 = silent, 1 = progress bar, 2 = one line per epoch.\n",
    "        Note that the progress bar is not particularly useful when logged to a file, so verbose=2 is recommended\n",
    "        when not running interactively (eg, in a production environment).\n",
    "        :param cer_mode: Turn on or off cer (Combined Experience Replay). Default is False.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        pass\n",
    "\n",
    "    @abstractmethod\n",
    "    def update_target_network(self):\n",
    "        \"\"\"\n",
    "        Update Q target Network by weight of Q network\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EpsilonGreedy:\n",
    "    def __init__(self, epsilon):\n",
    "        self.epsilon = epsilon\n",
    "\n",
    "    def perform(self, q_value, action_space: list = None):\n",
    "        prob = np.random.sample()  # get probability of taking random action\n",
    "        if prob <= self.epsilon:  # take random action\n",
    "            if action_space is None:  # all action are available\n",
    "                return np.random.randint(len(q_value))\n",
    "            return random.choice(action_space)\n",
    "        else:  # take greedy action\n",
    "            if action_space is None:\n",
    "                return np.argmax(q_value)\n",
    "            return max([[q_value[a], a] for a in action_space], key=lambda x: x[0])[1]\n",
    "\n",
    "    def decay(self, decay_value, lower_bound):\n",
    "        \"\"\"\n",
    "        Adjust the epsilon value by the formula: epsilon = max(decayValue * epsilon, lowerBound).\n",
    "        :param decay_value: Value ratio adjustment (0, 1).\n",
    "        :param lower_bound: Minimum epsilon value.\n",
    "        :return: None\n",
    "        \"\"\"\n",
    "        self.epsilon = max(self.epsilon * decay_value, lower_bound)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ExperienceReplay:\n",
    "    def __init__(self, e_max: int):\n",
    "        if e_max <= 0:\n",
    "            raise ValueError('Invalid value for memory size')\n",
    "        self.e_max = e_max\n",
    "        self.memory = list()\n",
    "        self.index = 0\n",
    "\n",
    "    def add_experience(self, sample: list):\n",
    "        if len(sample) != 5:\n",
    "            raise Exception('Invalid sample')\n",
    "        if len(self.memory) < self.e_max:\n",
    "            self.memory.append(sample)\n",
    "        else:\n",
    "            self.memory[self.index] = sample\n",
    "        self.index = (self.index + 1) % self.e_max\n",
    "\n",
    "    def sample_experience(self, sample_size: int, cer_mode: bool):\n",
    "        samples = random.sample(self.memory, sample_size)\n",
    "        if cer_mode:\n",
    "            samples[-1] = self.memory[self.index - 1]\n",
    "        # state_samples, action_samples, reward_samples, next_state_samples, done_samples\n",
    "        s_batch, a_batch, r_batch, ns_batch, done_batch = map(np.array, zip(*samples))\n",
    "        return s_batch, a_batch, r_batch, ns_batch, done_batch\n",
    "\n",
    "    def get_size(self):\n",
    "        return len(self.memory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DQN(BaseModel):\n",
    "    def __init__(self, discount_factor: float, epsilon: float, e_min: int, e_max: int):\n",
    "        super().__init__(discount_factor, epsilon, e_min, e_max)\n",
    "        self.gamma = discount_factor\n",
    "        self.epsilon_greedy = EpsilonGreedy(epsilon)\n",
    "        self.e_min = e_min\n",
    "        self.exp_replay = ExperienceReplay(e_max)\n",
    "        self.target_network = Sequential()\n",
    "        self.training_network = Sequential()\n",
    "        self.cache = list()\n",
    "\n",
    "    def observe(self, state, action_space: list = None):\n",
    "        q_value = self.training_network.predict(np.array([state])).ravel()\n",
    "        if action_space is not None:\n",
    "            return max([[q_value[a], a] for a in action_space], key=lambda x: x[0])[1]\n",
    "        return np.argmax(q_value)\n",
    "\n",
    "    def observe_on_training(self, state, action_space: list = None) -> int:\n",
    "        q_value = self.training_network.predict(np.array([state])).ravel()\n",
    "        action = self.epsilon_greedy.perform(q_value, action_space)\n",
    "        self.cache.extend([state, action])\n",
    "        return action\n",
    "\n",
    "    def take_reward(self, reward, next_state, done):\n",
    "        self.cache.extend([reward, next_state, done])\n",
    "        self.exp_replay.add_experience(self.cache.copy())\n",
    "        self.cache.clear()\n",
    "\n",
    "    def train_network(self, sample_size: int, batch_size: int, epochs: int, verbose: int = 2, cer_mode: bool = False):\n",
    "        if self.exp_replay.get_size() >= self.e_min:\n",
    "            # state_samples, action_samples, reward_samples, next_state_samples, done_samples\n",
    "            s_batch, a_batch, r_batch, ns_batch, done_batch = self.exp_replay.sample_experience(sample_size, cer_mode)\n",
    "            states, q_values = self.replay(s_batch, a_batch, r_batch, ns_batch, done_batch)\n",
    "            history = self.training_network.fit(states, q_values, epochs=epochs, batch_size=batch_size, verbose=verbose)\n",
    "            return history.history['loss']\n",
    "\n",
    "    def replay(self, states, actions, rewards, next_states, terminals):\n",
    "        q_values = self.target_network.predict(np.array(states))  # get q value at state t by target network\n",
    "        nq_values = self.target_network.predict(np.array(next_states))  # get q value at state t+1 by target network\n",
    "        for i in range(len(states)):\n",
    "            a = actions[i]\n",
    "            done = terminals[i]\n",
    "            r = rewards[i]\n",
    "            if done:\n",
    "                q_values[i][a] = r\n",
    "            else:\n",
    "                q_values[i][a] = r + self.gamma * np.max(nq_values[i])\n",
    "        return states, q_values\n",
    "\n",
    "    def update_target_network(self):\n",
    "        self.target_network.set_weights(self.training_network.get_weights())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from movement import *\n",
    "from reward import *\n",
    "from cmath import inf\n",
    "from Minmax import *\n",
    "\n",
    "\n",
    "pawnPromotions = ['r', 'n', 'q', 'b']\n",
    "\n",
    "\n",
    "def board(state):\n",
    "    \"\"\"\n",
    "    Board: Bàn cờ hiện tại\n",
    "    islower: lượt của quân viết thường (True) hay quân viết hoa (False)\n",
    "    return: giá trị của bàn cờ đối với quân viết thường\n",
    "    \"\"\"\n",
    "    board = [[0]*8 for _ in range(8)]\n",
    "    for y in range(8):\n",
    "        for x in range(8):\n",
    "            c = state[y][x]\n",
    "            if c == 0:\n",
    "                board[y][x] = ' '\n",
    "            elif abs(c) == 1:\n",
    "                board[y][x] = 'p' if c > 0 else 'P'\n",
    "            elif abs(c) == 3:\n",
    "                board[y][x] = 'n' if c > 0 else 'N'\n",
    "            elif abs(c) == 3.3:\n",
    "                board[y][x] = 'b' if c > 0 else 'B'\n",
    "            elif abs(c) == 5:\n",
    "                board[y][x] = 'r1' if c > 0 else 'R1'\n",
    "            elif abs(c) == 5.1:\n",
    "                board[y][x] = 'r2' if c > 0 else 'R2'\n",
    "            elif abs(c) == 20:\n",
    "                board[y][x] = 'q' if c > 0 else 'Q'\n",
    "            elif abs(c) == 1000:\n",
    "                board[y][x] = 'k' if c > 0 else 'K'\n",
    "            else:\n",
    "                print('Unknown piece:', c)\n",
    "    return board\n",
    "\n",
    "\n",
    "class Chess_Env:\n",
    "    def __init__(self):\n",
    "        self.board = [[\"r1\", \"n\", \"b\", \"q\", \"k\", \"b\", \"n\", \"r2\"],\n",
    "                      [\"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\"],\n",
    "                      [\" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \"],\n",
    "                      [\" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \"],\n",
    "                      [\" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \"],\n",
    "                      [\" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \"],\n",
    "                      [\"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\"],\n",
    "                      [\"R1\", \"N\", \"B\", \"Q\", \"K\", \"B\", \"N\", \"R2\"]]\n",
    "        self.isLower = True\n",
    "        self.isMoved = {'k': False, 'K': False, 'r1': False,\n",
    "                        'R1': False, 'r2': False, 'R2': False}\n",
    "        self.pos = {}\n",
    "        for row in range(8):\n",
    "            for col in range(8):\n",
    "                piece = self.board[row][col]\n",
    "                if piece != ' ':\n",
    "                    self.pos[piece] = (row, col)\n",
    "\n",
    "    def reset(self, isLowerFirst):\n",
    "        self.board = [[\"r1\", \"n\", \"b\", \"q\", \"k\", \"b\", \"n\", \"r2\"],\n",
    "                      [\"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\", \"p\"],\n",
    "                      [\" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \"],\n",
    "                      [\" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \"],\n",
    "                      [\" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \"],\n",
    "                      [\" \", \" \", \" \", \" \", \" \", \" \", \" \", \" \"],\n",
    "                      [\"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\", \"P\"],\n",
    "                      [\"R1\", \"N\", \"B\", \"Q\", \"K\", \"B\", \"N\", \"R2\"]]\n",
    "\n",
    "        self.isMoved = {'k': False, 'K': False, 'r1': False,\n",
    "                        'R1': False, 'r2': False, 'R2': False}\n",
    "        self.isLower = True\n",
    "        self.player_mark = 1 if isLowerFirst else 0\n",
    "        if isLowerFirst:\n",
    "            self.env_act()\n",
    "        return self.state.copy()\n",
    "\n",
    "    def check_win(self):\n",
    "        U, L = False, False\n",
    "        for _ in self.board:\n",
    "            U = U or ('K' in _)\n",
    "            L = L or ('k' in _)\n",
    "\n",
    "        if U and L:\n",
    "            return 0, False\n",
    "        elif U:\n",
    "            return 1000, True\n",
    "        return -1000, True\n",
    "\n",
    "    def env_act(self):\n",
    "        action = CPUMiniMaxTurn(self.board, self.isLower, self.isMoved, depth=2, position=self.pos)\n",
    "        row, col, newRow, newCol, pro = action\n",
    "        if self.isLower and 'p' in self.board[row][col] and row == 6:\n",
    "            pawnPromotion(self.board, row, col, newRow, newCol,\n",
    "                          random.choice(pawnPromotions))\n",
    "        elif not self.isLower and 'P' in self.board[row][col] and row == 1:\n",
    "            pawnPromotion(self.board, row, col, newRow, newCol,\n",
    "                          random.choice(pawnPromotions).upper())\n",
    "        else:\n",
    "            makeMove(self.board, row, col, newRow, newCol, self.isMoved)\n",
    "\n",
    "        self.isLower = not self.isLower\n",
    "        reward, done = self.check_win()\n",
    "        if done:\n",
    "            return reward, done\n",
    "        else:\n",
    "            reward = Reward(chessBoard, self.isLower)\n",
    "            return reward, done\n",
    "\n",
    "    def step(self, action, isMoved={}, pro=None):\n",
    "        row, col, newRow, newCol, p = action\n",
    "        curPiece = pro or self.board[row][col]\n",
    "        if self.pos.get(self.board[newRow][newCol]):\n",
    "            self.pos.pop(self.board[newRow][newCol])\n",
    "        self.board[newRow][newCol] = curPiece\n",
    "        self.pos[curPiece] = (newRow, newCol)\n",
    "        self.board[row][col] = ' '\n",
    "\n",
    "        if curPiece in 'kKR1r1R2r2':\n",
    "            isMoved[curPiece] = True\n",
    "\n",
    "        #  Quân trắng\n",
    "        if curPiece == 'K' and col - newCol == -2:\n",
    "            #  Di chuyển quân xe phải để nhập thành\n",
    "            row1, col1 = 7, 7\n",
    "            newRow, newCol = 7, 5\n",
    "            self.board[newRow][newCol] = self.board[row1][col1]\n",
    "            self.pos[self.board[row1][col1]] = (newRow, newCol)\n",
    "            self.board[row1][col1] = ' '\n",
    "\n",
    "        if curPiece == 'K' and col - newCol == 2:\n",
    "            #  Di chuyển quân xe trái để nhập thành\n",
    "            row1, col1 = 7, 0\n",
    "            newRow, newCol = 7, 3\n",
    "            self.board[newRow][newCol] = self.board[row1][col1]\n",
    "            self.pos[self.board[row1][col1]] = (newRow, newCol)\n",
    "            self.board[row1][col1] = ' '\n",
    "\n",
    "        #  Quân trắng\n",
    "        if curPiece == 'k' and col - newCol == -2:\n",
    "            #  Di chuyển quân xe phải để nhập thành\n",
    "            row1, col1 = 0, 7\n",
    "            newRow, newCol = 0, 2\n",
    "            self.board[newRow][newCol] = self.board[row1][col1]\n",
    "            self.pos[self.board[row1][col1]] = (newRow, newCol)\n",
    "            self.board[row1][col1] = ' '\n",
    "\n",
    "        if curPiece == 'k' and col - newCol == 2:\n",
    "            #  Di chuyển quân xe trái để nhập thành\n",
    "            row1, col1 = 0, 0\n",
    "            newRow, newCol = 0, 3\n",
    "            self.board[newRow][newCol] = self.board[row1][col1]\n",
    "            self.pos[self.board[row1][col1]] = (newRow, newCol)\n",
    "            self.board[row1][col1] = ' '\n",
    "\n",
    "        reward, done = self.check_win()       \n",
    "        self.isLower = not self.isLower\n",
    "\n",
    "        if done:\n",
    "            return self.state.copy(), reward, done, None\n",
    "        else:\n",
    "            reward = Reward(self.board, self.isLower)\n",
    "            return self.state.copy(), reward, done, None\n",
    "\n",
    "    @property\n",
    "    def state(self):\n",
    "        curState = [[0]*8 for _ in range(8)]\n",
    "        for y in range(8):\n",
    "            for x in range(8):\n",
    "                c = self.board[y][x]\n",
    "                if c == ' ':\n",
    "                    continue\n",
    "                if 'p' in c.lower():\n",
    "                    curState[y][x] = 1 if c.islower() else -1\n",
    "                elif 'n' in c.lower():\n",
    "                    curState[y][x] = 3 if c.islower() else -3\n",
    "                elif 'b' in c.lower():\n",
    "                    curState[y][x] = 3.3 if c.islower() else -3.3\n",
    "                elif 'r1' in c.lower():\n",
    "                    curState[y][x] = 5 if c.islower() else -5\n",
    "                elif 'r2' in c.lower():\n",
    "                    curState[y][x] = 5.1 if c.islower() else -5.1\n",
    "                elif 'q' in c.lower():\n",
    "                    curState[y][x] = 20 if c.islower() else -20\n",
    "                elif 'k' in c.lower():\n",
    "                    curState[y][x] = 1000 if c.islower() else -1000\n",
    "                else:\n",
    "                    print('Unknown piece:', c)\n",
    "        return curState\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Chess_Env()\n",
    "agent = DQN(0.7, 1, 4096, 1048576)\n",
    "op1 = optimizers.RMSprop(learning_rate=0.00025)\n",
    "agent.training_network.add(Dense(128, activation='relu', input_shape=(8,8,)))\n",
    "agent.training_network.add(Dense(128, activation='relu'))\n",
    "agent.training_network.add(Dense(64, activation='linear'))\n",
    "agent.training_network.compile(optimizer=op1, loss=losses.mean_squared_error)\n",
    "\n",
    "op2 = optimizers.RMSprop(learning_rate=0.00025)\n",
    "agent.target_network.add(Dense(128, activation='relu', input_shape=(8,8,)))\n",
    "agent.target_network.add(Dense(128, activation='relu'))\n",
    "agent.target_network.add(Dense(64, activation='linear'))\n",
    "agent.target_network.compile(optimizer=op2, loss=losses.mean_squared_error)\n",
    "reward_records = list()\n",
    "loss_records = list()\n",
    "count = 0\n",
    "tau = 100\n",
    "record = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total nodes 20\n",
      "Total nodes found in cache 0\n",
      "Time: 0.020867347717285156\n",
      "Max: 0.0\n",
      "1/1 [==============================] - 0s 173ms/step\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "list index out of range",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[35], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[39m# print(ep, '------------------', 'current epsilon: ', agent.epsilon_greedy.epsilon)\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[39mwhile\u001b[39;00m \u001b[39mnot\u001b[39;00m done:\n\u001b[0;32m----> 7\u001b[0m     action \u001b[39m=\u001b[39m agent\u001b[39m.\u001b[39;49mobserve_on_training(state,  CanGoList(board(state), isLower\u001b[39m=\u001b[39;49menv\u001b[39m.\u001b[39;49misLower))\n\u001b[1;32m      8\u001b[0m     state, reward, done, _ \u001b[39m=\u001b[39m env\u001b[39m.\u001b[39mstep(action, env\u001b[39m.\u001b[39misMoved)\n\u001b[1;32m      9\u001b[0m     \u001b[39m# print(state, done)\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[32], line 20\u001b[0m, in \u001b[0;36mDQN.observe_on_training\u001b[0;34m(self, state, action_space)\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mobserve_on_training\u001b[39m(\u001b[39mself\u001b[39m, state, action_space: \u001b[39mlist\u001b[39m \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mint\u001b[39m:\n\u001b[1;32m     19\u001b[0m     q_value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtraining_network\u001b[39m.\u001b[39mpredict(np\u001b[39m.\u001b[39marray([state]))\u001b[39m.\u001b[39mravel()\n\u001b[0;32m---> 20\u001b[0m     action \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mepsilon_greedy\u001b[39m.\u001b[39;49mperform(q_value, action_space)\n\u001b[1;32m     21\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcache\u001b[39m.\u001b[39mextend([state, action])\n\u001b[1;32m     22\u001b[0m     \u001b[39mreturn\u001b[39;00m action\n",
      "Cell \u001b[0;32mIn[30], line 10\u001b[0m, in \u001b[0;36mEpsilonGreedy.perform\u001b[0;34m(self, q_value, action_space)\u001b[0m\n\u001b[1;32m      8\u001b[0m     \u001b[39mif\u001b[39;00m action_space \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:  \u001b[39m# all action are available\u001b[39;00m\n\u001b[1;32m      9\u001b[0m         \u001b[39mreturn\u001b[39;00m np\u001b[39m.\u001b[39mrandom\u001b[39m.\u001b[39mrandint(\u001b[39mlen\u001b[39m(q_value))\n\u001b[0;32m---> 10\u001b[0m     \u001b[39mreturn\u001b[39;00m random\u001b[39m.\u001b[39;49mchoice(action_space)\n\u001b[1;32m     11\u001b[0m \u001b[39melse\u001b[39;00m:  \u001b[39m# take greedy action\u001b[39;00m\n\u001b[1;32m     12\u001b[0m     \u001b[39mif\u001b[39;00m action_space \u001b[39mis\u001b[39;00m \u001b[39mNone\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/lib/python3.10/random.py:378\u001b[0m, in \u001b[0;36mRandom.choice\u001b[0;34m(self, seq)\u001b[0m\n\u001b[1;32m    376\u001b[0m \u001b[39m\u001b[39m\u001b[39m\"\"\"Choose a random element from a non-empty sequence.\"\"\"\u001b[39;00m\n\u001b[1;32m    377\u001b[0m \u001b[39m# raises IndexError if seq is empty\u001b[39;00m\n\u001b[0;32m--> 378\u001b[0m \u001b[39mreturn\u001b[39;00m seq[\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49m_randbelow(\u001b[39mlen\u001b[39;49m(seq))]\n",
      "\u001b[0;31mIndexError\u001b[0m: list index out of range"
     ]
    }
   ],
   "source": [
    "if __name__ == '__main__':\n",
    "    for ep in range(100):\n",
    "        state = env.reset(isLowerFirst=env.isLower)\n",
    "        done = False\n",
    "        # print(ep, '------------------', 'current epsilon: ', agent.epsilon_greedy.epsilon)\n",
    "        while not done:\n",
    "            action = agent.observe_on_training(state,  CanGoList(board(state), isLower=env.isLower))\n",
    "            state, reward, done, _ = env.step(action, env.isMoved)\n",
    "            # print(state, done)\n",
    "            record += reward\n",
    "            print(ep, '-----------------------------------', reward)\n",
    "            agent.take_reward(reward, state, done)\n",
    "            hist = agent.train_network(64 ,64, 1, 2, cer_mode=True)\n",
    "            loss_records.append(hist)\n",
    "            count += 1\n",
    "            if count % tau == 0:\n",
    "                agent.update_target_network()\n",
    "        reward_records.append(record)\n",
    "        agent.epsilon_greedy.decay(0.99999, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(range(len(reward_records)),  reward_records)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('chess100.pkl', 'wb') as f:\n",
    "    pickle.dump(agent.training_network.get_weights(), f,pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
